#!/usr/bin/env python
"""í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ë§í‚¹ í…ŒìŠ¤íŠ¸.

ë²¡í„° ê²€ìƒ‰(ì˜ë¯¸ ìœ ì‚¬ë„) + BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ì„ ìœµí•©í•˜ì—¬ top-k í…Œì´ë¸”ì„ ì„ ì •í•©ë‹ˆë‹¤.

ì „ëµ:
1. Vector Search: ì§ˆì˜ ì„ë² ë”©ê³¼ í…Œì´ë¸”/ì»¬ëŸ¼ ì„¤ëª… ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
2. BM25 Search: í…Œì´ë¸”ëª…, ì»¬ëŸ¼ëª… í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰
3. Hybrid Fusion: ê°€ì¤‘ì¹˜ ìœµí•© (Vector Î± + BM25 (1-Î±))
4. Re-ranking: LLM ê¸°ë°˜ ì¬ìˆœìœ„í™” (ì˜µì…˜)

ì‚¬ìš©ë²•:
    python scripts/schema_linking_strategies/test_hybrid_search.py --test
    python scripts/schema_linking_strategies/test_hybrid_search.py --query "ìˆ˜ìœ¨ ë°ì´í„° ì¡°íšŒ"
"""

import argparse
import math
import re
import sys
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "scripts" / "schema_linking_strategies"))
sys.path.insert(0, str(PROJECT_ROOT / "src"))

from base import (
    MockSchemaDatabase,
    SchemaLinkingResult,
    EvaluationMetrics,
    TEST_CASES,
    TableInfo,
    extract_keywords,
    print_result_table,
    compute_aggregate_metrics,
)


# ============================================================================
# ê°„ë‹¨í•œ ì„ë² ë”© ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì‹¤ì œ ì„ë² ë”© ì‚¬ìš©)
# ============================================================================


class SimpleEmbedding:
    """ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜ ì„ë² ë”© ì‹œë®¬ë ˆì´ì…˜.
    
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” OpenAI/ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤.
    """

    def __init__(self):
        self.vocab: dict[str, int] = {}
        self.idf: dict[str, float] = {}
        self.docs: list[list[str]] = []

    def fit(self, documents: list[str]) -> None:
        """ë¬¸ì„œë¡œë¶€í„° ì–´íœ˜ì™€ IDF ê³„ì‚°."""
        self.docs = [self._tokenize(doc) for doc in documents]
        
        # ì–´íœ˜ êµ¬ì¶•
        all_words = set()
        for tokens in self.docs:
            all_words.update(tokens)
        self.vocab = {word: i for i, word in enumerate(sorted(all_words))}
        
        # IDF ê³„ì‚°
        n_docs = len(self.docs)
        for word in self.vocab:
            doc_count = sum(1 for tokens in self.docs if word in tokens)
            self.idf[word] = math.log((n_docs + 1) / (doc_count + 1)) + 1

    def embed(self, text: str) -> list[float]:
        """í…ìŠ¤íŠ¸ë¥¼ TF-IDF ë²¡í„°ë¡œ ë³€í™˜."""
        tokens = self._tokenize(text)
        tf = Counter(tokens)
        
        vector = [0.0] * len(self.vocab)
        for word, count in tf.items():
            if word in self.vocab:
                idx = self.vocab[word]
                vector[idx] = count * self.idf.get(word, 1.0)
        
        # L2 ì •ê·œí™”
        norm = math.sqrt(sum(v * v for v in vector))
        if norm > 0:
            vector = [v / norm for v in vector]
        
        return vector

    def _tokenize(self, text: str) -> list[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”."""
        words = re.findall(r"[ê°€-í£]+|[A-Za-z0-9_]+", text.lower())
        return words

    @staticmethod
    def cosine_similarity(vec1: list[float], vec2: list[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°."""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(b * b for b in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)


# ============================================================================
# BM25 ê²€ìƒ‰
# ============================================================================


class BM25:
    """BM25 í‚¤ì›Œë“œ ê²€ìƒ‰."""

    def __init__(self, k1: float = 1.5, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.docs: list[list[str]] = []
        self.doc_lengths: list[int] = []
        self.avg_doc_len: float = 0.0
        self.idf: dict[str, float] = {}

    def fit(self, documents: list[str]) -> None:
        """ë¬¸ì„œ ì¸ë±ì‹±."""
        self.docs = [self._tokenize(doc) for doc in documents]
        self.doc_lengths = [len(doc) for doc in self.docs]
        self.avg_doc_len = sum(self.doc_lengths) / len(self.docs) if self.docs else 0
        
        # IDF ê³„ì‚°
        n_docs = len(self.docs)
        all_words = set()
        for tokens in self.docs:
            all_words.update(tokens)
        
        for word in all_words:
            doc_count = sum(1 for tokens in self.docs if word in tokens)
            self.idf[word] = math.log((n_docs - doc_count + 0.5) / (doc_count + 0.5) + 1)

    def score(self, query: str, doc_idx: int) -> float:
        """ì¿¼ë¦¬ì™€ ë¬¸ì„œ ê°„ BM25 ì ìˆ˜ ê³„ì‚°."""
        query_tokens = self._tokenize(query)
        doc_tokens = self.docs[doc_idx]
        doc_len = self.doc_lengths[doc_idx]
        
        score = 0.0
        tf = Counter(doc_tokens)
        
        for term in query_tokens:
            if term not in self.idf:
                continue
            
            term_freq = tf.get(term, 0)
            idf = self.idf[term]
            
            numerator = term_freq * (self.k1 + 1)
            denominator = term_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len)
            
            score += idf * numerator / denominator
        
        return score

    def search(self, query: str, top_k: int = 10) -> list[tuple[int, float]]:
        """ì¿¼ë¦¬ë¡œ ë¬¸ì„œ ê²€ìƒ‰."""
        scores = [(i, self.score(query, i)) for i in range(len(self.docs))]
        scores.sort(key=lambda x: -x[1])
        return scores[:top_k]

    def _tokenize(self, text: str) -> list[str]:
        """í…ìŠ¤íŠ¸ í† í°í™”."""
        words = re.findall(r"[ê°€-í£]+|[A-Za-z0-9_]+", text.lower())
        return words


# ============================================================================
# í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìŠ¤í‚¤ë§ˆ ë§ì»¤
# ============================================================================


class HybridSchemaLinker:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ë§ì»¤.
    
    Vector + BM25 ìœµí•©ìœ¼ë¡œ í…Œì´ë¸”ì„ ì„ ì •í•©ë‹ˆë‹¤.
    """

    def __init__(
        self,
        schema_db: MockSchemaDatabase,
        vector_weight: float = 0.7,
        use_reranking: bool = False,
    ):
        self.schema_db = schema_db
        self.vector_weight = vector_weight
        self.bm25_weight = 1.0 - vector_weight
        self.use_reranking = use_reranking
        
        self.embedding = SimpleEmbedding()
        self.bm25 = BM25()
        self.table_names: list[str] = []
        self.table_docs: list[str] = []
        
        self._build_index()

    def _build_index(self) -> None:
        """í…Œì´ë¸” ì¸ë±ìŠ¤ êµ¬ì¶•."""
        self.table_names = []
        self.table_docs = []
        
        for table in self.schema_db.get_all_tables():
            self.table_names.append(table.name)
            
            # í…Œì´ë¸” ë¬¸ì„œ ìƒì„±: ì´ë¦„ + ì„¤ëª… + ì»¬ëŸ¼ ì •ë³´
            doc_parts = [
                table.name,
                table.description,
                table.purpose,
            ]
            for col in table.columns:
                doc_parts.extend([col.name, col.description])
                doc_parts.extend(col.sample_values)
            
            self.table_docs.append(" ".join(doc_parts))
        
        # ì„ë² ë”© ë° BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
        self.embedding.fit(self.table_docs)
        self.bm25.fit(self.table_docs)

    def link(self, query: str, top_k: int = 5) -> SchemaLinkingResult:
        """ì§ˆì˜ì— ëŒ€í•œ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ìˆ˜í–‰.
        
        Args:
            query: ìì—°ì–´ ì§ˆì˜
            top_k: ë°˜í™˜í•  ìƒìœ„ í…Œì´ë¸” ìˆ˜
            
        Returns:
            SchemaLinkingResult
        """
        # 1. Vector Search
        query_vec = self.embedding.embed(query)
        vector_scores = {}
        
        for i, table_name in enumerate(self.table_names):
            table_vec = self.embedding.embed(self.table_docs[i])
            score = SimpleEmbedding.cosine_similarity(query_vec, table_vec)
            vector_scores[table_name] = score

        # 2. BM25 Search
        bm25_results = self.bm25.search(query, top_k=len(self.table_names))
        bm25_scores = {self.table_names[idx]: score for idx, score in bm25_results}
        
        # ì ìˆ˜ ì •ê·œí™”
        max_bm25 = max(bm25_scores.values()) if bm25_scores else 1.0
        if max_bm25 > 0:
            bm25_scores = {k: v / max_bm25 for k, v in bm25_scores.items()}

        # 3. Hybrid Fusion
        hybrid_scores = {}
        for table_name in self.table_names:
            vec_score = vector_scores.get(table_name, 0.0)
            bm25_score = bm25_scores.get(table_name, 0.0)
            hybrid_scores[table_name] = (
                self.vector_weight * vec_score + self.bm25_weight * bm25_score
            )

        # 4. Re-ranking (ì˜µì…˜)
        if self.use_reranking:
            hybrid_scores = self._rerank(query, hybrid_scores)

        # ìƒìœ„ kê°œ ì„ íƒ
        sorted_tables = sorted(hybrid_scores.items(), key=lambda x: -x[1])
        selected = [t[0] for t in sorted_tables[:top_k]]

        return SchemaLinkingResult(
            query=query,
            selected_tables=selected,
            scores=hybrid_scores,
        )

    def _rerank(self, query: str, scores: dict[str, float]) -> dict[str, float]:
        """LLM ê¸°ë°˜ ì¬ìˆœìœ„í™” (Mock).
        
        ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” LLMì„ í˜¸ì¶œí•˜ì—¬ ì¬ìˆœìœ„í™”í•©ë‹ˆë‹¤.
        ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ ì ìˆ˜ì— ì•½ê°„ì˜ ì¡°ì •ë§Œ ì ìš©í•©ë‹ˆë‹¤.
        """
        # Mock: ì§ˆì˜ì— í¬í•¨ëœ í‚¤ì›Œë“œê°€ í…Œì´ë¸”ëª…ì— ìˆìœ¼ë©´ ì ìˆ˜ ë¶€ìŠ¤íŠ¸
        keywords = extract_keywords(query)
        
        reranked = {}
        for table, score in scores.items():
            boost = 0.0
            for kw in keywords:
                if kw.upper() in table.upper():
                    boost += 0.1
            reranked[table] = min(score + boost, 1.0)
        
        return reranked


# ============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================================================


def run_tests() -> None:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰."""
    print("=" * 80)
    print("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ë§í‚¹ í…ŒìŠ¤íŠ¸")
    print("=" * 80)
    
    # ìŠ¤í‚¤ë§ˆ DB ë° ë§ì»¤ ì´ˆê¸°í™”
    schema_db = MockSchemaDatabase()
    
    # ê°€ì¤‘ì¹˜ ë³€í™”ì— ë”°ë¥¸ ì„±ëŠ¥ ë¹„êµ
    weight_configs = [
        (1.0, 0.0, "Vector Only"),
        (0.0, 1.0, "BM25 Only"),
        (0.7, 0.3, "Hybrid (0.7:0.3)"),
        (0.5, 0.5, "Hybrid (0.5:0.5)"),
    ]
    
    for vec_w, bm25_w, config_name in weight_configs:
        print(f"\n\n{'='*40}")
        print(f"ì„¤ì •: {config_name}")
        print(f"{'='*40}")
        
        linker = HybridSchemaLinker(
            schema_db=schema_db,
            vector_weight=vec_w,
            use_reranking=False,
        )
        
        results = []
        for test_case in TEST_CASES:
            result = linker.link(test_case["query"], top_k=5)
            result.ground_truth = test_case["ground_truth"]
            results.append(result)
        
        # ê²°ê³¼ ì¶œë ¥
        print_result_table(results)
        
        # ì§‘ê³„ ì§€í‘œ
        aggregate = compute_aggregate_metrics(results)
        print("\nğŸ“Š ì§‘ê³„ ì§€í‘œ:")
        for metric, value in aggregate.items():
            print(f"  {metric}: {value:.4f}")


def run_single_query(query: str, vector_weight: float = 0.7, use_reranking: bool = False) -> None:
    """ë‹¨ì¼ ì§ˆì˜ í…ŒìŠ¤íŠ¸."""
    print(f"\nì§ˆì˜: {query}")
    print(f"ê°€ì¤‘ì¹˜: Vector={vector_weight}, BM25={1-vector_weight}")
    print(f"Re-ranking: {'í™œì„±í™”' if use_reranking else 'ë¹„í™œì„±í™”'}")
    print("-" * 60)
    
    schema_db = MockSchemaDatabase()
    linker = HybridSchemaLinker(
        schema_db=schema_db,
        vector_weight=vector_weight,
        use_reranking=use_reranking,
    )
    
    result = linker.link(query, top_k=5)
    
    print("\nì„ ì •ëœ í…Œì´ë¸”:")
    for i, table in enumerate(result.selected_tables, 1):
        score = result.scores.get(table, 0.0)
        print(f"  {i}. {table} (score: {score:.4f})")
    
    # í…Œì´ë¸” ìƒì„¸ ì •ë³´
    print("\ní…Œì´ë¸” ìƒì„¸:")
    for table_name in result.selected_tables[:3]:
        table = schema_db.get_table(table_name)
        if table:
            print(f"\n  ğŸ“‹ {table.name}")
            print(f"     ì„¤ëª…: {table.description}")
            print(f"     ì»¬ëŸ¼: {', '.join(table.column_names[:5])}...")


def main():
    """ë©”ì¸ í•¨ìˆ˜."""
    parser = argparse.ArgumentParser(
        description="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ë§í‚¹ í…ŒìŠ¤íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--test",
        action="store_true",
        help="ì „ì²´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰",
    )
    parser.add_argument(
        "--query",
        type=str,
        default=None,
        help="í…ŒìŠ¤íŠ¸í•  ìì—°ì–´ ì§ˆì˜",
    )
    parser.add_argument(
        "--vector-weight",
        type=float,
        default=0.7,
        help="ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.7)",
    )
    parser.add_argument(
        "--rerank",
        action="store_true",
        help="LLM ê¸°ë°˜ ì¬ìˆœìœ„í™” í™œì„±í™”",
    )
    
    args = parser.parse_args()
    
    if args.test:
        run_tests()
    elif args.query:
        run_single_query(args.query, args.vector_weight, args.rerank)
    else:
        # ê¸°ë³¸: ê°„ë‹¨í•œ ë°ëª¨
        print("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìŠ¤í‚¤ë§ˆ ë§í‚¹ ë°ëª¨")
        print("-" * 40)
        run_single_query("M10 íŒ¹ì˜ ìˆ˜ìœ¨ ë°ì´í„°ë¥¼ ë³´ì—¬ì¤˜")
        run_single_query("ì„¤ë¹„ë³„ ìƒì‚°ëŸ‰ê³¼ ë¶ˆëŸ‰ë¥ ")


if __name__ == "__main__":
    main()
